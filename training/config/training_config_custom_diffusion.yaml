# configs/training_config.yaml

# -------------------
# File & Directory Paths
# -------------------
data:
  meta_data_path: "../dset/new.json"
  audio_data_root: "/mnt/gestalt/home/ndrw1221/datasets/pili/pili_no_vocals_47s"
  audio_latent_root: "/mnt/gestalt/home/ndrw1221/datasets/pili/pili_no_vocals_47s_latent"
  
# -------------------
# Output & Logging
# -------------------
output:
  output_dir_root: "/mnt/gestalt/home/ndrw1221/sao_pili-output/output"
  use_wandb: true
  wandb_project_name: "stable-audio-pili"
  run_name_prefix: "cus_diff"
  checkpointing_steps: 100
  validation_steps: 100
  
# -------------------
# Training Parameters
# -------------------
training:
  val_ratio: 0 # No validation set, use the same caption for validation
  train_batch_size: 5
  gradient_accumulation_steps: 1 # No gradient accumulation
  learning_rate: 8.0e-5
  weight_decay: 1.0e-3
  num_train_epochs: 1000 # The number of steps is around this value times 25
  max_train_steps: 1000 # If null, calculated from num_train_epochs
  resume_from_checkpoint: null # e.g., "/path/to/checkpoint-4000"

# -------------------
# LoRA Parameters (if use_lora is true)
# -------------------
lora:
  use_lora: false
  r: 32
  alpha: 96
  target_modules: ["to_q", "to_k", "to_v", "to_out.0"]
  lora_dropout: 0.05

# -------------------
# PEFT Parameters (if use_lora is false)
# -------------------
peft:
  cross_kv: true # Only train W_k, W_v in cross-attention layers

# -------------------
# Dataloader Parameters
# -------------------
dataloader:
  val_batch_size: 5
  num_workers: 4

# -------------------
# LR Scheduler Parameters
# -------------------
lr_scheduler:
  type: "constant" # Options: "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup", "piecewise_constant"
  num_warmup_steps: 0
  power: 3.5 # For polynomial scheduler
  num_cycles: 10 # For cosine_with_restarts scheduler

# -------------------
# Pipeline & Validation Parameters
# -------------------
validation:
  denoise_steps: 100
  num_samples_to_log_per_batch: 1