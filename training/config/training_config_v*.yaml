# configs/training_config.yaml

# -------------------
# File & Directory Paths
# -------------------
data:
  meta_data_path: "../dset/pili_no_vocals_47s_v*.json"
  audio_data_root: "/mnt/gestalt/home/ndrw1221/datasets/pili/pili_no_vocals_47s"
  audio_latent_root: "/mnt/gestalt/home/ndrw1221/datasets/pili/pili_no_vocals_47s_latent"
  
# -------------------
# Output & Logging
# -------------------
output:
  output_dir_root: "/mnt/gestalt/home/ndrw1221/sao_pili-output/output"
  use_wandb: true
  wandb_project_name: "stable-audio-pili"
  run_name_prefix: "v*"
  checkpointing_steps: 1000
  validation_steps: 1000
  
# -------------------
# Training Parameters
# -------------------
training:
  val_ratio: 0.1
  train_batch_size: 6
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-5
  weight_decay: 1.0e-3
  num_train_epochs: 1000 # The number of steps is around this value times 25
  max_train_steps: 5000 # If null, calculated from num_train_epochs
  resume_from_checkpoint: null # e.g., "/path/to/checkpoint-4000"

# -------------------
# LoRA Parameters (if use_lora is true)
# -------------------
lora:
  use_lora: true
  r: 32
  alpha: 96
  target_modules: ["to_q", "to_k", "to_v", "to_out.0"]
  lora_dropout: 0.05

# -------------------
# Dataloader Parameters
# -------------------
dataloader:
  val_batch_size: 5
  num_workers: 4

# -------------------
# LR Scheduler Parameters
# -------------------
lr_scheduler:
  type: "polynomial" # Options: "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup", "piecewise_constant"
  num_warmup_steps: 100
  power: 3.0 # For polynomial scheduler
  num_cycles: 10 # For cosine_with_restarts scheduler

# -------------------
# Pipeline & Validation Parameters
# -------------------
validation:
  denoise_steps: 100
  num_samples_to_log_per_batch: 2